## CUDA torch functions for LLM

For study purpose

implemented attentions
- Naive Attention
- Attention with KV
- Attention with non-contagious memory
- Attention with non-contagious KV cache (PagedAttention with block size 1)
- Rotary Embedding